{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce852316-e1c6-45bb-bffb-bc5900c6b561",
   "metadata": {},
   "source": [
    "# Spectral Clustering Experiment Notebook\n",
    "\n",
    "This notebook runs spectral clustering experiments using both SimKit (via Neo4j) and scikit-learn.\n",
    "\n",
    "We use a simple dataset stored in `datasets/points.csv` with these columns:\n",
    "\n",
    "| id  | x_coordinate | y_coordinate | class |\n",
    "|-----|--------------|--------------|-------|\n",
    "| p1  | 1            | 7            | 1     |\n",
    "| p2  | 1            | 6            | 1     |\n",
    "| p3  | 6            | 2            | 2     |\n",
    "| p4  | 8            | 1            | 2     |\n",
    "| p5  | 10           | 2            | 2     |\n",
    "\n",
    "Make sure the CSV file is present in the `datasets/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050674e6-0839-4a7b-a895-5834ddeae535",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j is already installed.\n",
      "pandas is already installed.\n",
      "psutil is already installed.\n",
      "tqdm is already installed.\n",
      "Installing scikit-learn...\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "scipy is already installed.\n",
      "matplotlib is already installed.\n",
      "seaborn is already installed.\n",
      "All required packages are imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse.csgraph import laplacian as csgraph_laplacian\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "# Visualization imports\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure required packages are installed\n",
    "required_packages = [\"neo4j\", \"pandas\", \"psutil\", \"tqdm\", \"scikit-learn\", \"scipy\", \"matplotlib\", \"seaborn\"]\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Verify imports\n",
    "print(\"All required packages are imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de917df4-66f5-4c5c-880b-16079c0ffb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:50.185011Z",
     "start_time": "2025-04-08T15:04:50.158962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j connection successful: 1\n"
     ]
    }
   ],
   "source": [
    "# Neo4j connection details\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"123412345\"\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "#driver.close()\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1\")\n",
    "        print(\"Neo4j connection successful:\", result.single()[0])\n",
    "except Exception as e:\n",
    "    print(f\"Neo4j connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63eb5cb-aec5-45fb-b360-228d43cd5200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:50.621556Z",
     "start_time": "2025-04-08T15:04:50.257198Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating sigmas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 113359.57it/s]\n",
      "Building full affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 93414.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: [(np.int64(7), array([3, 4, 0, 0, 5, 1, 1, 2, 2, 6], dtype=int32), np.float64(0.19888725879400854))]\n"
     ]
    }
   ],
   "source": [
    "def check_symmetric(matrix, tol=1e-8):\n",
    "    return np.allclose(matrix, matrix.T, atol=tol)\n",
    "\n",
    "def spectral_clustering(dataframe, similarity_graph, laplacian, number_of_clusters, eps=None, k=None):\n",
    "    dimension = dataframe.shape[0]\n",
    "    dist_mat = squareform(pdist(dataframe))\n",
    "    sample_size = len(dist_mat)\n",
    "    n = min(sample_size // 10, int(math.log(sample_size)))\n",
    "    epsilon = eps if eps else np.percentile(dist_mat, 90)\n",
    "    k = k if k else int(np.sqrt(sample_size))\n",
    "    \n",
    "    if similarity_graph == \"full\":\n",
    "        sigmas = np.zeros(dimension)\n",
    "        for i in tqdm(range(len(dist_mat)), desc=\"Calculating sigmas\"):\n",
    "            sigmas[i] = sorted(dist_mat[i])[n]\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building full affinity\"):\n",
    "            for j in range(i+1, dimension):\n",
    "                d = np.exp(-1 * dist_mat[i, j]**2 / (sigmas[i] * sigmas[j]))\n",
    "                adjacency_matrix[i, j] = d\n",
    "                adjacency_matrix[j, i] = d\n",
    "    elif similarity_graph == \"eps\":\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building eps affinity\"):\n",
    "            for j in range(i+1, dimension):\n",
    "                d = 1 if dist_mat[i, j] < epsilon else 0\n",
    "                adjacency_matrix[i, j] = d\n",
    "                adjacency_matrix[j, i] = d\n",
    "    elif similarity_graph == \"knn\":\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building knn affinity\"):\n",
    "            sorted_indices = np.argsort(dist_mat[i])\n",
    "            k_nearest_indices = sorted_indices[1:k+1]\n",
    "            adjacency_matrix[i, k_nearest_indices] = 1\n",
    "    else:  # mknn\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building mknn affinity\"):\n",
    "            sorted_indices = np.argsort(dist_mat[i])\n",
    "            k_nearest_indices = sorted_indices[1:k+1]\n",
    "            for neighbor in k_nearest_indices:\n",
    "                neighbor_sorted_indices = np.argsort(dist_mat[neighbor])\n",
    "                if i in neighbor_sorted_indices[1:k+1]:\n",
    "                    adjacency_matrix[i, neighbor] = 1\n",
    "                    adjacency_matrix[neighbor, i] = 1\n",
    "\n",
    "    degrees = np.sum(adjacency_matrix, axis=1)\n",
    "    degree_matrix = np.diag(degrees)\n",
    "\n",
    "    if laplacian == \"sym\":\n",
    "        d_inv_sqrt = np.zeros_like(degrees)\n",
    "        nonzero = degrees > 0\n",
    "        d_inv_sqrt[nonzero] = 1.0 / np.sqrt(degrees[nonzero])\n",
    "        d_half = np.diag(d_inv_sqrt)\n",
    "        laplacian_matrix_normalized = d_half @ adjacency_matrix @ d_half\n",
    "    elif laplacian == \"rw\":\n",
    "        d_inv = np.zeros_like(degrees)\n",
    "        nonzero = degrees > 0\n",
    "        d_inv[nonzero] = 1.0 / degrees[nonzero]\n",
    "        d_inverse = np.diag(d_inv)\n",
    "        laplacian_matrix_normalized = d_inverse @ adjacency_matrix\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported laplacian type. Only 'sym' and 'rw' are allowed.\")\n",
    "\n",
    "    if check_symmetric(laplacian_matrix_normalized):\n",
    "        e, v = np.linalg.eigh(laplacian_matrix_normalized)\n",
    "    else:\n",
    "        e, v = np.linalg.eig(laplacian_matrix_normalized)\n",
    "        idx = np.argsort(np.real(e))\n",
    "        e = np.real(e[idx])\n",
    "        v = np.real(v[:, idx])\n",
    "    \n",
    "    eigengap = np.diff(e)\n",
    "    optimal_number_of_clusters = np.argmax(eigengap[:10]) + 1\n",
    "\n",
    "    if number_of_clusters == \"fixed2\":\n",
    "        current_k = 2\n",
    "    elif number_of_clusters == \"fixed3\":\n",
    "        current_k = 3\n",
    "    else:\n",
    "        current_k = max(optimal_number_of_clusters, 2)\n",
    "\n",
    "    X = v[:, -current_k:]\n",
    "    clustering = KMeans(n_clusters=current_k, random_state=42, n_init=100)\n",
    "    cluster_labels = clustering.fit_predict(X)\n",
    "\n",
    "    sil_score = silhouette_score(dataframe, cluster_labels)\n",
    "    return [(current_k, cluster_labels, sil_score)]\n",
    "\n",
    "# Test the function with dummy data\n",
    "dummy_data = pd.DataFrame(np.random.rand(10, 2), columns=['x', 'y'])\n",
    "result = spectral_clustering(dummy_data, \"full\", \"sym\", 2)\n",
    "print(\"Test result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dad2120-f5b8-4345-b16a-b0371a5c111f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:50.735197Z",
     "start_time": "2025-04-08T15:04:50.640013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 451 nodes.\n",
      "Deleted 0 nodes.\n",
      "Dropped index: IrisNode_id_index\n",
      "Dropped index: IrisNode_label_index\n",
      "Dropped index: affinity_full_11_IrisNode_Index_idx\n",
      "Dropped index: eigen_sym_3_affinity_full_11_IrisNode_Index_idx\n"
     ]
    }
   ],
   "source": [
    "def ensure_indexes(driver, datasets):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            existing_indexes = session.run(\"SHOW INDEXES\")\n",
    "            existing_index_names = {record[\"name\"] for record in existing_indexes}\n",
    "            for dataset, params in datasets.items():\n",
    "                node_label = params.get(\"label\") or params.get(\"node_label\")\n",
    "                index_id_name = f\"{node_label}_id_index\"\n",
    "                index_label_name = f\"{node_label}_label_index\"\n",
    "                if index_id_name not in existing_index_names:\n",
    "                    session.run(f\"CREATE INDEX {index_id_name} FOR (n:{node_label}) ON (n.id);\")\n",
    "                if index_label_name not in existing_index_names:\n",
    "                    session.run(f\"CREATE INDEX {index_label_name} FOR (n:{node_label}) ON (n.label);\")\n",
    "        print(\"‚úÖ Indexes ensured for all datasets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error creating indexes: {e}\")\n",
    "\n",
    "def delete_all_nodes(driver, batch_size=1000):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            while True:\n",
    "                result = session.run(f\"MATCH (n) WITH n LIMIT {batch_size} DETACH DELETE n RETURN count(n) AS deleted_count\")\n",
    "                deleted_count = result.single()[\"deleted_count\"]\n",
    "                print(f\"Deleted {deleted_count} nodes.\")\n",
    "                if deleted_count == 0:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error during node deletion:\", e)\n",
    "\n",
    "def delete_all_indexes(driver, batch_size=5):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            while True:\n",
    "                indexes = session.run(\"CALL db.indexes()\")\n",
    "                index_names = [index[\"name\"] for index in indexes]\n",
    "                if not index_names:\n",
    "                    break\n",
    "                for index_name in index_names[:batch_size]:\n",
    "                    session.run(f\"DROP INDEX {index_name}\")\n",
    "                    print(f\"Dropped index: {index_name}\")\n",
    "                if len(index_names) <= batch_size:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error during index deletion:\", e)\n",
    "\n",
    "# Test deletion\n",
    "delete_all_nodes(driver)\n",
    "delete_all_indexes(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a17ae17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:51.054549Z",
     "start_time": "2025-04-08T15:04:51.048434Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_nodes(data, driver, label):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            for _, row in data.iterrows():\n",
    "                properties = { (f\"feature_{key}\" if str(key).isdigit() else key): value\n",
    "                               for key, value in row.items() }\n",
    "                query = f\"CREATE (n:{label} {{\" + ', '.join([f\"{key}: ${key}\" for key in properties.keys()]) + \"})\"\n",
    "                session.run(query, **properties)\n",
    "        print(f\"Created feature nodes with label {label}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during node creation:\", e)\n",
    "\n",
    "def create_graph_nodes(data, driver, label):\n",
    "    try:\n",
    "        node_data = pd.read_csv(data)\n",
    "        with driver.session() as session:\n",
    "            for _, row in node_data.iterrows():\n",
    "                properties = row.to_dict()\n",
    "                properties['features'] = eval(properties['features'])\n",
    "                query = f\"CREATE (n:{label} {{id: $id, features: $features, label: $label}})\"\n",
    "                session.run(query, **properties)\n",
    "        print(f\"Created graph nodes with label {label}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during node creation:\", e)\n",
    "\n",
    "def create_edges(data, driver, node_label, edge_label):\n",
    "    try:\n",
    "        edge_data = pd.read_csv(data)\n",
    "        with driver.session() as session:\n",
    "            for _, row in edge_data.iterrows():\n",
    "                source_id = min(row['source_id'], row['target_id'])\n",
    "                target_id = max(row['source_id'], row['target_id'])\n",
    "                query = f\"\"\"\n",
    "                MATCH (source:{node_label} {{id: $source_id}})\n",
    "                MATCH (target:{node_label} {{id: $target_id}})\n",
    "                MERGE (source)-[:{edge_label} {{value: 1}}]->(target)\n",
    "                \"\"\"\n",
    "                session.run(query, {\"source_id\": source_id, \"target_id\": target_id})\n",
    "        print(f\"Created edges with label {edge_label}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during edge creation:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ac9ee7-50a6-439a-969c-5fb5b56da5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:51.730274Z",
     "start_time": "2025-04-08T15:04:51.724815Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_query(driver, query, parameters):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu_times = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "    with driver.session() as session:\n",
    "        print(\"Parameters:\")\n",
    "        print(parameters)\n",
    "        result = session.run(query, parameters)\n",
    "        record = result.single()\n",
    "        data = record.data() if record else None\n",
    "    end_time = time.time()\n",
    "    end_cpu_times = process.cpu_times()\n",
    "    end_mem = process.memory_info().rss\n",
    "    duration = end_time - start_time\n",
    "    cpu_used = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return data, duration, memory_used, cpu_used\n",
    "#local_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "def monitor_progress():\n",
    "    #local_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    while True:\n",
    "        with driver.session() as session:\n",
    "            query = \"MATCH (p:Progress {id: 'current'}) RETURN p.step ORDER BY p.timestamp DESC LIMIT 1\"\n",
    "            result = session.run(query)\n",
    "            record = result.single()\n",
    "            data = record.data() if record else None\n",
    "        if data:\n",
    "            try:\n",
    "                print(f\"üîÑ Current Step: {data['p.step']}\", flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}, Result: {data}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# Start monitoring thread\n",
    "#monitor_thread = threading.Thread(target=monitor_progress, daemon=True)\n",
    "#monitor_thread.start()\n",
    "#print(\"Monitoring thread started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15291a0-1379-43c9-84d2-7110eca7e965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:52.202358Z",
     "start_time": "2025-04-08T15:04:52.194744Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_sklearn_experiment_feature(config, file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    cols_to_remove = [col.strip() for col in config[\"remove_columns\"].split(',')]\n",
    "    features = df.drop(columns=cols_to_remove, errors='ignore')\n",
    "    true_labels = df[config[\"target_column\"]].values\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "\n",
    "    eps_val = float(config[\"parameter\"]) if config[\"graph_type\"] == \"eps\" else None\n",
    "    k_val = int(config[\"parameter\"]) if config[\"graph_type\"] in [\"knn\", \"mknn\"] else None\n",
    "\n",
    "    clustering_result = spectral_clustering(features, config[\"graph_type\"], config[\"laplacian_type\"],\n",
    "                                            config[\"number_of_eigenvectors\"], eps=eps_val, k=k_val)\n",
    "    current_k, cluster_labels, sil_score = clustering_result[0]\n",
    "\n",
    "    clustering_time = time.time() - start_time\n",
    "    skl_silhouette = sil_score\n",
    "    skl_rand_index = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    end_cpu = process.cpu_times()\n",
    "    cpu_used = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "    end_mem = process.memory_info().rss\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return {\n",
    "        \"sklearn_silhouette_score\": skl_silhouette,\n",
    "        \"sklearn_rand_index\": skl_rand_index,\n",
    "        \"sklearn_total_time\": clustering_time,\n",
    "        \"sklearn_memory_used\": memory_used,\n",
    "        \"sklearn_cpu_used\": cpu_used\n",
    "    }\n",
    "\n",
    "def run_sklearn_experiment_graph(config, node_file_path, edge_file_path):\n",
    "    nodes_df = pd.read_csv(node_file_path)\n",
    "    true_labels = nodes_df[config[\"target_column\"]].values\n",
    "    features = nodes_df.drop(columns=[col.strip() for col in config[\"remove_columns\"].split(',')], errors='ignore')\n",
    "    if \"features\" in features.columns:\n",
    "        features = np.array(features[\"features\"].apply(lambda x: eval(x) if isinstance(x, str) else x).tolist())\n",
    "    else:\n",
    "        features = features.values.astype(float)\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "\n",
    "    clustering_result = spectral_clustering(features, config[\"graph_type\"], config[\"laplacian_type\"],\n",
    "                                            config[\"number_of_eigenvectors\"])\n",
    "    current_k, cluster_labels, sil_score = clustering_result[0]\n",
    "\n",
    "    clustering_time = time.time() - start_time\n",
    "    skl_silhouette = sil_score\n",
    "    skl_rand_index = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    end_cpu = process.cpu_times()\n",
    "    cpu_used = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "    end_mem = process.memory_info().rss\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return {\n",
    "        \"sklearn_silhouette_score\": skl_silhouette,\n",
    "        \"sklearn_rand_index\": skl_rand_index,\n",
    "        \"sklearn_total_time\": clustering_time,\n",
    "        \"sklearn_memory_used\": memory_used,\n",
    "        \"sklearn_cpu_used\": cpu_used\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcebfdd5-3c0c-4ffa-af13-9a34cc2e0f7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:52.845489Z",
     "start_time": "2025-04-08T15:04:52.840335Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiments(driver, experiments):\n",
    "    print(\"Initializing SimKit...\")\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"RETURN simkit.initSimKit('bolt://localhost:7687', 'neo4j', '123412345')\")\n",
    "        print(\"SimKit initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing SimKit: {e}\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for idx, config in enumerate(tqdm(experiments, desc=\"Running experiments\")):\n",
    "        print(\"Config:\", config)\n",
    "        query = \"\"\"\n",
    "        WITH simkit.experimental_spectralClustering({\n",
    "            node_label: $node_label,\n",
    "            is_feature_based: $is_feature_based,\n",
    "            distance_measure: \"euclidean\",\n",
    "            graph_type: $graph_type,\n",
    "            parameter: $parameter,\n",
    "            remove_columns: $remove_columns,\n",
    "            laplacian_type: $laplacian_type,\n",
    "            number_of_eigenvectors: $number_of_eigenvectors,\n",
    "            number_of_iterations: 100,\n",
    "            distance_measure_kmean: \"euclidean\",\n",
    "            target_column: $target_column,\n",
    "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
    "            seed: 42 \n",
    "        }) AS result\n",
    "        RETURN result.silhouette_score AS silhouette_score, \n",
    "               result.rand_index AS rand_index,\n",
    "               result.total_time AS total_time\n",
    "        \"\"\"\n",
    "        print(query)\n",
    "        data, duration, memory_used, cpu_used = run_query(driver, query, config)\n",
    "        simkit_result = {\n",
    "            \"silhouette_score\": data['silhouette_score'] if data else None,\n",
    "            \"rand_index\": data['rand_index'] if data else None,\n",
    "            \"total_time\": data['total_time'] if data else duration,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"cpu_used\": cpu_used\n",
    "        }\n",
    "        \n",
    "        if config.get(\"is_feature_based\"):\n",
    "            file_path = os.path.join(\"datasets\", f\"points.csv\")\n",
    "            sklearn_result = run_sklearn_experiment_feature(config, file_path)\n",
    "        else:\n",
    "            node_file_path = os.path.join(\"datasets\", f\"{config['node_label'].replace('Node','').lower()}_nodes.csv\")\n",
    "            edge_file_path = os.path.join(\"datasets\", f\"{config['node_label'].replace('Node','').lower()}_edges.csv\")\n",
    "            sklearn_result = run_sklearn_experiment_graph(config, node_file_path, edge_file_path)\n",
    "        \n",
    "        results.append({**config, **simkit_result, **sklearn_result})\n",
    "        print(f\"Completed experiment {idx+1}/{len(experiments)}\")\n",
    "    return results\n",
    "\n",
    "def save_results(results, dataset):\n",
    "    df = pd.DataFrame(results)\n",
    "    results_dir = \"results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    df.to_csv(os.path.join(results_dir, f\"{dataset}_results.csv\"), index=False)\n",
    "    print(f\"Results saved to {dataset}_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bc8414-c6eb-49e1-8c05-ab35ffb0e9a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:53.554370Z",
     "start_time": "2025-04-08T15:04:53.547603Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_feature_experiment(dataset, label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"label\": label}})\n",
    "    file_path = os.path.join(\"datasets\", f\"{dataset}.csv\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    create_feature_nodes(data, driver, label)\n",
    "    \n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "    parameters = {\"iris\": {\"full\": \"11\", \"eps\": \"1.111\", \"knn\": \"10\", \"mknn\": \"30\"},\n",
    "                  \"madelon\": {\"full\": \"45\", \"eps\": \"4.669\", \"knn\": \"419\", \"mknn\": \"117\"},\n",
    "                  \"20newsgroups\": {\"full\": \"35\", \"eps\": \"1946.74\", \"knn\": \"512\", \"mknn\": \"26\"}}\n",
    "    for graph_type in graph_types:\n",
    "        for laplacian_type in laplacian_types:\n",
    "            experiments.append({\n",
    "                \"node_label\": label,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[dataset][graph_type],\n",
    "                \"remove_columns\": remove_columns,\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"target_column\": target_column,\n",
    "                \"use_kmean_for_silhouette\": False\n",
    "            })\n",
    "    results = run_experiments(driver, experiments)\n",
    "    save_results(results, dataset)\n",
    "\n",
    "def run_graph_experiment(dataset, node_label, edge_label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"node_label\": node_label}})\n",
    "    node_file_path = os.path.join(\"datasets\", f\"{dataset}_nodes.csv\")\n",
    "    edge_file_path = os.path.join(\"datasets\", f\"{dataset}_edges.csv\")\n",
    "    create_graph_nodes(node_file_path, driver, node_label)\n",
    "    create_edges(edge_file_path, driver, node_label, edge_label)\n",
    "    \n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    for laplacian_type in laplacian_types:\n",
    "        experiments.append({\n",
    "            \"node_label\": node_label,\n",
    "            \"is_feature_based\": False,\n",
    "            \"graph_type\": \"full\",\n",
    "            \"parameter\": \"3\",\n",
    "            \"remove_columns\": remove_columns,\n",
    "            \"laplacian_type\": laplacian_type,\n",
    "            \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "            \"target_column\": target_column,\n",
    "            \"use_kmean_for_silhouette\": True\n",
    "        })\n",
    "        print({\n",
    "            \"node_label\": node_label,\n",
    "            \"is_feature_based\": False,\n",
    "            \"graph_type\": \"full\",\n",
    "            \"parameter\": \"3\",\n",
    "            \"remove_columns\": remove_columns,\n",
    "            \"laplacian_type\": laplacian_type,\n",
    "            \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "            \"target_column\": target_column,\n",
    "            \"use_kmean_for_silhouette\": True\n",
    "        })\n",
    "    results = run_experiments(driver, experiments)\n",
    "    save_results(results, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b129e633-f856-4e77-b0ed-2365c0c6f2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:04:56.390549Z",
     "start_time": "2025-04-08T15:04:54.795424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 27 nodes.\n",
      "Deleted 0 nodes.\n",
      "Dropped index: PointNode_id_index\n",
      "Dropped index: PointNode_label_index\n",
      "Dropped index: affinity_full_5_PointNode_id_idx\n",
      "Dropped index: eigen_sym_2_affinity_full_5_PointNode_id_idx\n",
      "‚úÖ Indexes ensured for all datasets.\n",
      "Dataset loaded successfully:\n",
      "   id  x_coordinate  y_coordinate  class\n",
      "0   1             1             7      1\n",
      "1   2             1             6      1\n",
      "2   3             6             2      2\n",
      "3   4             8             1      2\n",
      "4   5            10             2      2\n",
      "Created feature nodes with label PointNode\n",
      "Initializing SimKit...\n",
      "SimKit initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running experiments:   0%|                                                                                                       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'full', 'parameter': '5', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'full', 'parameter': '5', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating sigmas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 61832.49it/s]\u001b[A\n",
      "\n",
      "Building full affinity:   0%|                                                                                                    | 0/6 [00:00<?, ?it/s]\u001b[A/var/folders/5p/rmxkrxk95qg8wghypx84dpw80000gn/T/ipykernel_1685/518358290.py:19: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  d = np.exp(-1 * dist_mat[i, j]**2 / (sigmas[i] * sigmas[j]))\n",
      "Building full affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 29468.18it/s]\n",
      "Running experiments:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                               | 1/6 [00:01<00:08,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 1/6\n",
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'full', 'parameter': '5', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'full', 'parameter': '5', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating sigmas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 66576.25it/s]\u001b[A\n",
      "\n",
      "Building full affinity:   0%|                                                                                                    | 0/6 [00:00<?, ?it/s]\u001b[A/var/folders/5p/rmxkrxk95qg8wghypx84dpw80000gn/T/ipykernel_1685/518358290.py:19: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  d = np.exp(-1 * dist_mat[i, j]**2 / (sigmas[i] * sigmas[j]))\n",
      "Building full affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 35746.91it/s]\n",
      "Running experiments:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                               | 2/6 [00:03<00:06,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 2/6\n",
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'eps', 'parameter': '2.0', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'eps', 'parameter': '2.0', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building eps affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 12294.00it/s]\u001b[A\n",
      "Running experiments:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                               | 3/6 [00:04<00:04,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 3/6\n",
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'eps', 'parameter': '2.0', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'eps', 'parameter': '2.0', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building eps affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 75121.86it/s]\u001b[A\n",
      "Running experiments:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 4/6 [00:06<00:03,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 4/6\n",
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'knn', 'parameter': '2', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'knn', 'parameter': '2', 'remove_columns': 'id,class', 'laplacian_type': 'sym', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building knn affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 7832.50it/s]\u001b[A\n",
      "Running experiments:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 5/6 [00:07<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 5/6\n",
      "Config: {'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'knn', 'parameter': '2', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n",
      "\n",
      "        WITH simkit.experimental_spectralClustering({\n",
      "            node_label: $node_label,\n",
      "            is_feature_based: $is_feature_based,\n",
      "            distance_measure: \"euclidean\",\n",
      "            graph_type: $graph_type,\n",
      "            parameter: $parameter,\n",
      "            remove_columns: $remove_columns,\n",
      "            laplacian_type: $laplacian_type,\n",
      "            number_of_eigenvectors: $number_of_eigenvectors,\n",
      "            number_of_iterations: 100,\n",
      "            distance_measure_kmean: \"euclidean\",\n",
      "            target_column: $target_column,\n",
      "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
      "            seed: 42 \n",
      "        }) AS result\n",
      "        RETURN result.silhouette_score AS silhouette_score, \n",
      "               result.rand_index AS rand_index,\n",
      "               result.total_time AS total_time\n",
      "        \n",
      "Parameters:\n",
      "{'node_label': 'PointNode', 'is_feature_based': True, 'graph_type': 'knn', 'parameter': '2', 'remove_columns': 'id,class', 'laplacian_type': 'rw', 'number_of_eigenvectors': 2, 'target_column': 'class', 'use_kmean_for_silhouette': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building knn affinity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 9108.15it/s]\u001b[A\n",
      "Running experiments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:09<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed experiment 6/6\n",
      "Results saved to points_results.csv\n",
      "Experiment completed for points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset based on points.csv\n",
    "feature_datasets = {\n",
    "    \"points\": {\n",
    "        \"label\": \"PointNode\",\n",
    "        \"remove_columns\": \"id,class\",\n",
    "        \"number_of_eigenvectors\": 2,  # Integer\n",
    "        \"target_column\": \"class\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_feature_experiment(dataset, label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"label\": label}})\n",
    "    file_path = os.path.join(\"datasets\", f\"{dataset}.csv\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(\"Dataset loaded successfully:\")\n",
    "        print(data.head())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please ensure it exists in the datasets/ folder.\")\n",
    "        return\n",
    "    \n",
    "    create_feature_nodes(data, driver, label)\n",
    "    \n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    graph_types = [\"full\", \"eps\", \"knn\"]\n",
    "    default_parameters = {\n",
    "        \"full\": \"5\",    # String\n",
    "        \"eps\": \"2.0\",   # String\n",
    "        \"knn\": \"2\"      # String\n",
    "    }\n",
    "    \n",
    "    for graph_type in graph_types:\n",
    "        for laplacian_type in laplacian_types:\n",
    "            experiments.append({\n",
    "                \"node_label\": label,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": default_parameters[graph_type],\n",
    "                \"remove_columns\": remove_columns,\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,  # Integer\n",
    "                \"target_column\": target_column,\n",
    "                \"use_kmean_for_silhouette\": False\n",
    "            })\n",
    "    \n",
    "    results = run_experiments(driver, experiments)\n",
    "    save_results(results, dataset)\n",
    "    print(f\"Experiment completed for {dataset}\")\n",
    "\n",
    "run_feature_experiment(\"points\", **feature_datasets[\"points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a46a963b-5f26-4395-94b8-6852c38c7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc7c05-5926-4f2a-b091-50820e6237ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and analyze results\n",
    "result_files = glob.glob(os.path.join(\"results\", \"points_results.csv\"))\n",
    "if not result_files:\n",
    "    print(\"No results found. Ensure the experiment ran successfully.\")\n",
    "else:\n",
    "    combined_df = pd.read_csv(result_files[0])\n",
    "    print(\"Results loaded:\")\n",
    "    print(combined_df.head())\n",
    "\n",
    "    # Average metrics\n",
    "    simkit_avg = combined_df[['total_time', 'cpu_used', 'silhouette_score', 'rand_index']].mean()\n",
    "    sklearn_avg = combined_df[['sklearn_total_time', 'sklearn_cpu_used', 'sklearn_silhouette_score', 'sklearn_rand_index']].mean()\n",
    "    print(\"SimKit Averages:\\n\", simkit_avg)\n",
    "    print(\"scikit-learn Averages:\\n\", sklearn_avg)\n",
    "\n",
    "    # Scatter plot of points with true and predicted labels\n",
    "    data = pd.read_csv(\"datasets/points.csv\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # True labels\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(data['x_coordinate'], data['y_coordinate'], c=data['class'], cmap='viridis')\n",
    "    plt.title(\"True Labels\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    \n",
    "    # Predicted labels (SimKit)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Assuming cluster labels are not directly returned, we'll simulate them for visualization\n",
    "    # For actual labels, you'd need to modify run_experiments to return them\n",
    "    plt.scatter(data['x_coordinate'], data['y_coordinate'], c=combined_df['silhouette_score'].iloc[0], cmap='viridis')\n",
    "    plt.title(\"SimKit Predicted Clusters (Silhouette Score as Proxy)\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot for total time comparison\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.boxplot(data=pd.DataFrame({\n",
    "        'SimKit Total Time': combined_df['total_time'],\n",
    "        'scikit-learn Total Time': combined_df['sklearn_total_time']\n",
    "    }))\n",
    "    plt.title(\"Total Time Comparison\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f8a1c-2069-4520-98e2-82709ab47bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657879d-8163-41fb-be2b-489934ced8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c8557-53cc-452f-b9b1-bc2856626c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050674e6-0839-4a7b-a895-5834ddeae535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "\n",
    "# Ensure required packages are installed\n",
    "required_packages = [\"neo4j\", \"pandas\", \"psutil\", \"tqdm\", \"scikit-learn\", \"scipy\"]\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# For scikit-learn experiments\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse.csgraph import laplacian as csgraph_laplacian\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de917df4-66f5-4c5c-880b-16079c0ffb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions and spectral_clustering defined.\n"
     ]
    }
   ],
   "source": [
    "def check_symmetric(matrix, tol=1e-8):\n",
    "    return np.allclose(matrix, matrix.T, atol=tol)\n",
    "\n",
    "def spectral_clustering(dataframe, similarity_graph, laplacian, number_of_clusters, eps=None, k=None):\n",
    "    # Pairwise distances\n",
    "    dimension = dataframe.shape[0]\n",
    "    dist_mat = squareform(pdist(dataframe))\n",
    "    sample_size = len(dist_mat)\n",
    "    \n",
    "    # Set n based on proportional selection, but limit by log scaling for large datasets\n",
    "    n = min(sample_size // 10, int(math.log(sample_size)))\n",
    "    \n",
    "    # Fallback values for epsilon and k\n",
    "    epsilon = eps if eps else np.percentile(dist_mat, 90)\n",
    "    k = k if k else int(np.sqrt(sample_size))\n",
    "    \n",
    "    if similarity_graph == \"full\":\n",
    "        # Calculate local sigma\n",
    "        sigmas = np.zeros(dimension)\n",
    "        for i in tqdm(range(len(dist_mat)), desc=\"Calculating sigmas\"):\n",
    "            sigmas[i] = sorted(dist_mat[i])[n]\n",
    "        # Build adjacency matrix with optimal sigma\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building full affinity\"):\n",
    "            for j in range(i+1, dimension):\n",
    "                d = np.exp(-1 * dist_mat[i, j]**2 / (sigmas[i] * sigmas[j]))\n",
    "                adjacency_matrix[i, j] = d\n",
    "                adjacency_matrix[j, i] = d\n",
    "    elif similarity_graph == \"eps\":\n",
    "        # Build adjacency matrix with epsilon threshold\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building eps affinity\"):\n",
    "            for j in range(i+1, dimension):\n",
    "                d = 1 if dist_mat[i, j] < epsilon else 0\n",
    "                adjacency_matrix[i, j] = d\n",
    "                adjacency_matrix[j, i] = d\n",
    "    elif similarity_graph == \"knn\":\n",
    "        # Build adjacency matrix with k-neighbours\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building knn affinity\"):\n",
    "            sorted_indices = np.argsort(dist_mat[i])\n",
    "            k_nearest_indices = sorted_indices[1:k+1]  # Exclude self\n",
    "            adjacency_matrix[i, k_nearest_indices] = 1\n",
    "    else:\n",
    "        # Build adjacency matrix with mutual k-neighbours (for \"mknn\")\n",
    "        adjacency_matrix = np.zeros([dimension, dimension])\n",
    "        for i in tqdm(range(dimension), desc=\"Building mknn affinity\"):\n",
    "            sorted_indices = np.argsort(dist_mat[i])\n",
    "            k_nearest_indices = sorted_indices[1:k+1]\n",
    "            for neighbor in k_nearest_indices:\n",
    "                neighbor_sorted_indices = np.argsort(dist_mat[neighbor])\n",
    "                if i in neighbor_sorted_indices[1:k+1]:\n",
    "                    adjacency_matrix[i, neighbor] = 1\n",
    "                    adjacency_matrix[neighbor, i] = 1\n",
    "\n",
    "    # Calculate degree matrix\n",
    "    degrees = np.sum(adjacency_matrix, axis=1)\n",
    "    if laplacian == \"sym\":\n",
    "        # Normalized Symmetric Laplacian matrix\n",
    "        d_inv_sqrt = np.zeros_like(degrees)\n",
    "        nonzero = degrees > 0\n",
    "        d_inv_sqrt[nonzero] = 1.0 / np.sqrt(degrees[nonzero])\n",
    "        d_half = np.diag(d_inv_sqrt)\n",
    "        laplacian_matrix_normalized = d_half @ adjacency_matrix @ d_half\n",
    "    elif laplacian == \"rw\":\n",
    "        # Normalized Random Walk Laplacian matrix\n",
    "        d_inv = np.zeros_like(degrees)\n",
    "        nonzero = degrees > 0\n",
    "        d_inv[nonzero] = 1.0 / degrees[nonzero]\n",
    "        d_inverse = np.diag(d_inv)\n",
    "        laplacian_matrix_normalized = d_inverse @ adjacency_matrix\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported laplacian type. Only 'sym' and 'rw' are allowed.\")\n",
    "    \n",
    "    if check_symmetric(laplacian_matrix_normalized):\n",
    "        e, v = np.linalg.eigh(laplacian_matrix_normalized)\n",
    "    else:\n",
    "        e, v = np.linalg.eig(laplacian_matrix_normalized)\n",
    "        idx = np.argsort(np.real(e))\n",
    "        e = np.real(e[idx])\n",
    "        v = np.real(v[:, idx])\n",
    "    \n",
    "    # Calculate eigengap and determine optimal number of clusters\n",
    "    eigengap = np.diff(e)\n",
    "    optimal_number_of_clusters = np.argmax(eigengap[:10]) + 1\n",
    "\n",
    "    if number_of_clusters == \"fixed2\":\n",
    "        current_k = 2\n",
    "    elif number_of_clusters == \"fixed3\":\n",
    "        current_k = 3\n",
    "    else:\n",
    "        current_k = max(optimal_number_of_clusters, 2)\n",
    "\n",
    "    # Perform clustering on the eigenvectors\n",
    "    X = v[:, -current_k:]\n",
    "    clustering = KMeans(n_clusters=current_k, random_state=42, n_init=100)\n",
    "    cluster_labels = clustering.fit_predict(X)\n",
    "    sil_score = silhouette_score(dataframe, cluster_labels)\n",
    "    return [(current_k, cluster_labels, sil_score)]\n",
    "\n",
    "print(\"Helper functions and spectral_clustering defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78fc866-a359-4912-9b1f-d9857f84a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j connection and node/edge functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Neo4j connection details\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"123412345\"\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def ensure_indexes(driver, datasets):\n",
    "    \"\"\"Ensures indexes exist for each dataset automatically.\"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            existing_indexes = session.run(\"SHOW INDEXES\")\n",
    "            existing_index_names = {record[\"name\"] for record in existing_indexes}\n",
    "            for dataset, params in datasets.items():\n",
    "                node_label = params.get(\"label\") or params.get(\"node_label\")\n",
    "                index_id_name = f\"{node_label}_id_index\"\n",
    "                index_label_name = f\"{node_label}_label_index\"\n",
    "                if index_id_name not in existing_index_names:\n",
    "                    session.run(f\"CREATE INDEX {index_id_name} FOR (n:{node_label}) ON (n.id);\")\n",
    "                if index_label_name not in existing_index_names:\n",
    "                    session.run(f\"CREATE INDEX {index_label_name} FOR (n:{node_label}) ON (n.label);\")\n",
    "        print(\"✅ Indexes ensured for all datasets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error creating indexes: {e}\")\n",
    "\n",
    "def delete_all_nodes(driver, batch_size=1000):\n",
    "    \"\"\"Delete all nodes and relationships in the Neo4j database in batches.\"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            while True:\n",
    "                result = session.run(f\"MATCH (n) WITH n LIMIT {batch_size} DETACH DELETE n RETURN count(n) AS deleted_count\")\n",
    "                deleted_count = result.single()[\"deleted_count\"]\n",
    "                if deleted_count == 0:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error during node deletion:\", e)\n",
    "\n",
    "def delete_all_indexes(driver, batch_size=5):\n",
    "    \"\"\"Delete all indexes in the Neo4j database in batches.\"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            while True:\n",
    "                indexes = session.run(\"CALL db.indexes()\")\n",
    "                index_names = [index[\"name\"] for index in indexes]\n",
    "                if not index_names:\n",
    "                    break\n",
    "                for index_name in index_names[:batch_size]:\n",
    "                    session.run(f\"DROP INDEX {index_name}\")\n",
    "                if len(index_names) <= batch_size:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error during index deletion:\", e)\n",
    "\n",
    "def create_feature_nodes(data, driver, label):\n",
    "    \"\"\"Create feature-based nodes in Neo4j from a CSV file.\"\"\"\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            for _, row in data.iterrows():\n",
    "                properties = { (f\"feature_{key}\" if str(key).isdigit() else key): value\n",
    "                               for key, value in row.items() }\n",
    "                query = f\"CREATE (n:{label} {{\" + ', '.join([f\"{key}: ${key}\" for key in properties.keys()]) + \"})\"\n",
    "                session.run(query, **properties)\n",
    "    except Exception as e:\n",
    "        print(\"Error during node creation:\", e)\n",
    "\n",
    "def create_graph_nodes(data, driver, label):\n",
    "    \"\"\"Create graph-based nodes in Neo4j from a CSV file.\"\"\"\n",
    "    try:\n",
    "        node_data = pd.read_csv(data)\n",
    "        with driver.session() as session:\n",
    "            for _, row in node_data.iterrows():\n",
    "                properties = row.to_dict()\n",
    "                properties['features'] = eval(properties['features'])\n",
    "                query = f\"CREATE (n:{label} {{id: $id, features: $features, label: $label}})\"\n",
    "                session.run(query, **properties)\n",
    "    except Exception as e:\n",
    "        print(\"Error during node creation:\", e)\n",
    "\n",
    "def create_edges(data, driver, node_label, edge_label):\n",
    "    \"\"\"Create undirected edges in Neo4j from a CSV file.\"\"\"\n",
    "    try:\n",
    "        edge_data = pd.read_csv(data)\n",
    "        with driver.session() as session:\n",
    "            for _, row in edge_data.iterrows():\n",
    "                source_id = min(row['source_id'], row['target_id'])\n",
    "                target_id = max(row['source_id'], row['target_id'])\n",
    "                query = f\"\"\"\n",
    "                MATCH (source:{node_label} {{id: $source_id}})\n",
    "                MATCH (target:{node_label} {{id: $target_id}})\n",
    "                MERGE (source)-[:{edge_label} {{value: 1}}]->(target)\n",
    "                \"\"\"\n",
    "                session.run(query, {\"source_id\": source_id, \"target_id\": target_id})\n",
    "    except Exception as e:\n",
    "        print(\"Error during edge creation:\", e)\n",
    "\n",
    "def run_query(driver, query, parameters):\n",
    "    \"\"\"Run a query and measure performance metrics.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu_times = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters)\n",
    "        record = result.single()\n",
    "        data = record.data() if record else None\n",
    "    end_time = time.time()\n",
    "    end_cpu_times = process.cpu_times()\n",
    "    end_mem = process.memory_info().rss\n",
    "    duration = end_time - start_time\n",
    "    cpu_used = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return data, duration, memory_used, cpu_used\n",
    "\n",
    "def monitor_progress():\n",
    "    \"\"\"Continuously fetch progress updates while the main query runs.\"\"\"\n",
    "    local_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    while True:\n",
    "        with local_driver.session() as session:\n",
    "            query = \"MATCH (p:Progress {id: 'current'}) RETURN p.step ORDER BY p.timestamp DESC LIMIT 1\"\n",
    "            result = session.run(query)\n",
    "            record = result.single()\n",
    "            data = record.data() if record else None\n",
    "        if data:\n",
    "            try:\n",
    "                print(f\"🔄 Current Step: {data['p.step']}\", flush=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}, Result: {data}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# Optionally, start the monitoring thread (if you wish to see progress updates)\n",
    "monitor_thread = threading.Thread(target=monitor_progress, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "print(\"Neo4j connection and node/edge functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63eb5cb-aec5-45fb-b360-228d43cd5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment functions defined.\n"
     ]
    }
   ],
   "source": [
    "def run_sklearn_experiment_feature(config, file_path):\n",
    "    \"\"\"Run scikit-learn spectral clustering on feature-based data.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Start with the remove_columns from config (which is now empty for points)\n",
    "    cols_to_remove = [col.strip() for col in config[\"remove_columns\"].split(',') if col.strip() != \"\"]\n",
    "    # If this is the PointNode case, drop \"id\" so that it is not used as a feature.\n",
    "    if config.get(\"node_label\", \"\") == \"PointNode\" and \"id\" in df.columns:\n",
    "        cols_to_remove.append(\"id\")\n",
    "    features = df.drop(columns=cols_to_remove, errors='ignore')\n",
    "    true_labels = df[config[\"target_column\"]].values\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "\n",
    "    if config[\"graph_type\"] == \"eps\":\n",
    "        eps_val = float(config[\"parameter\"])\n",
    "        k_val = None\n",
    "    elif config[\"graph_type\"] in [\"knn\", \"mknn\"]:\n",
    "        k_val = int(config[\"parameter\"])\n",
    "        eps_val = None\n",
    "    else:\n",
    "        eps_val = None\n",
    "        k_val = None\n",
    "\n",
    "    clustering_result = spectral_clustering(features, config[\"graph_type\"], config[\"laplacian_type\"],\n",
    "                                            config[\"number_of_eigenvectors\"], eps=eps_val, k=k_val)\n",
    "    current_k, cluster_labels, sil_score = clustering_result[0]\n",
    "    clustering_time = time.time() - start_time\n",
    "    skl_silhouette = sil_score\n",
    "    skl_rand_index = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    total_time = clustering_time\n",
    "    end_cpu = process.cpu_times()\n",
    "    cpu_used = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "    end_mem = process.memory_info().rss\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return {\n",
    "        \"sklearn_silhouette_score\": skl_silhouette,\n",
    "        \"sklearn_rand_index\": skl_rand_index,\n",
    "        \"sklearn_total_time\": total_time,\n",
    "        \"sklearn_affinity_time\": 0,\n",
    "        \"sklearn_laplacian_time\": 0,\n",
    "        \"sklearn_clustering_time\": clustering_time,\n",
    "        \"sklearn_adjusted_rand_index_time\": 0,\n",
    "        \"sklearn_memory_used\": memory_used,\n",
    "        \"sklearn_cpu_used\": cpu_used\n",
    "    }\n",
    "\n",
    "\n",
    "def run_sklearn_experiment_graph(config, node_file_path, edge_file_path):\n",
    "    \"\"\"Run scikit-learn spectral clustering on graph-based data.\"\"\"\n",
    "    nodes_df = pd.read_csv(node_file_path)\n",
    "    true_labels = nodes_df[config[\"target_column\"]].values\n",
    "    features = nodes_df.drop(columns=[col.strip() for col in config[\"remove_columns\"].split(',')], errors='ignore')\n",
    "\n",
    "    # Convert 'features' column if exists\n",
    "    if \"features\" in features.columns:\n",
    "        features = np.array(features[\"features\"].apply(lambda x: eval(x) if isinstance(x, str) else x).tolist())\n",
    "    else:\n",
    "        features = features.values.astype(float)\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss\n",
    "\n",
    "    eps_val = None\n",
    "    k_val = None\n",
    "    clustering_result = spectral_clustering(features, config[\"graph_type\"], config[\"laplacian_type\"],\n",
    "                                            config[\"number_of_eigenvectors\"], eps=eps_val, k=k_val)\n",
    "    current_k, cluster_labels, sil_score = clustering_result[0]\n",
    "    clustering_time = time.time() - start_time\n",
    "    skl_silhouette = sil_score\n",
    "    skl_rand_index = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    total_time = clustering_time\n",
    "    end_cpu = process.cpu_times()\n",
    "    cpu_used = (end_cpu.user + end_cpu.system) - (start_cpu.user + start_cpu.system)\n",
    "    end_mem = process.memory_info().rss\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)\n",
    "    return {\n",
    "        \"sklearn_silhouette_score\": skl_silhouette,\n",
    "        \"sklearn_rand_index\": skl_rand_index,\n",
    "        \"sklearn_total_time\": total_time,\n",
    "        \"sklearn_affinity_time\": 0,\n",
    "        \"sklearn_laplacian_time\": 0,\n",
    "        \"sklearn_clustering_time\": clustering_time,\n",
    "        \"sklearn_adjusted_rand_index_time\": 0,\n",
    "        \"sklearn_memory_used\": memory_used,\n",
    "        \"sklearn_cpu_used\": cpu_used\n",
    "    }\n",
    "\n",
    "def run_experiments(driver, experiments):\n",
    "    \"\"\"Run experiments using both SimKit and scikit-learn.\"\"\"\n",
    "    print(\"Initializing SimKit...\")\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"\"\"\n",
    "            RETURN simkit.initSimKit('bolt://localhost:7687', 'neo4j', '123412345')\n",
    "            \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing SimKit: {e}\")\n",
    "        return\n",
    "    print(\"SimKit initialized.\")\n",
    "    results = []\n",
    "    total_experiments = len(experiments)\n",
    "    for idx, config in enumerate(experiments, 1):\n",
    "        query = \"\"\"\n",
    "        WITH simkit.experimental_spectralClustering({\n",
    "            node_label: $node_label,\n",
    "            is_feature_based: $is_feature_based,\n",
    "            distance_measure: \"euclidean\",\n",
    "            graph_type: $graph_type,\n",
    "            parameter: $parameter,\n",
    "            remove_columns: $remove_columns,\n",
    "            laplacian_type: $laplacian_type,\n",
    "            number_of_eigenvectors: $number_of_eigenvectors,\n",
    "            number_of_iterations: \"100\",\n",
    "            distance_measure_kmean: \"euclidean\",\n",
    "            target_column: $target_column,\n",
    "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
    "            seed: \"42\"\n",
    "        }) AS result\n",
    "        RETURN result.silhouette_score AS silhouette_score, \n",
    "               result.rand_index AS rand_index,\n",
    "               result.total_time AS total_time,\n",
    "               result.affinity_time AS affinity_time,\n",
    "               result.laplacian_time AS laplacian_time,\n",
    "               result.clustering_time AS clustering_time,\n",
    "               result.adjusted_rand_index_time AS adjusted_rand_index_time\n",
    "        \"\"\"\n",
    "        data, duration, memory_used, cpu_used = run_query(driver, query, config)\n",
    "        simkit_result = {\n",
    "            \"silhouette_score\": data['silhouette_score'] if data else None,\n",
    "            \"rand_index\": data['rand_index'] if data else None,\n",
    "            \"total_time\": data['total_time'] if data else duration,\n",
    "            \"affinity_time\": data['affinity_time'] if data else None,\n",
    "            \"laplacian_time\": data['laplacian_time'] if data else None,\n",
    "            \"clustering_time\": data['clustering_time'] if data else None,\n",
    "            \"adjusted_rand_index_time\": data['adjusted_rand_index_time'] if data else None,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"cpu_used\": cpu_used\n",
    "        }\n",
    "        \n",
    "        if config.get(\"is_feature_based\"):\n",
    "            sklearn_result = run_sklearn_experiment_feature(\n",
    "                config, \n",
    "                os.path.join(\"datasets\", f\"{config['node_label'].replace('Node','').lower()}.csv\")\n",
    "            )\n",
    "        else:\n",
    "            node_file_path = os.path.join(\"datasets\", f\"{config['node_label'].replace('Node','').lower()}_nodes.csv\")\n",
    "            edge_file_path = os.path.join(\"datasets\", f\"{config['node_label'].replace('Node','').lower()}_edges.csv\")\n",
    "            sklearn_result = run_sklearn_experiment_graph(config, node_file_path, edge_file_path)\n",
    "        \n",
    "        merged_result = {**config, **simkit_result, **sklearn_result}\n",
    "        results.append(merged_result)\n",
    "        print(f\"Completed experiment {idx}/{total_experiments} with config: {config}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def save_results(results, dataset):\n",
    "    \"\"\"Save experiment results to a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    results_dir = \"results\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    output_file = os.path.join(results_dir, f\"{dataset}_results.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "def run_feature_experiment(dataset, label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    \"\"\"Run feature-based experiments.\"\"\"\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"label\": label}})\n",
    "    file_path = os.path.join(\"datasets\", f\"{dataset}.csv\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    create_feature_nodes(data, driver, label)\n",
    "    print(f\"Feature nodes created for {dataset}\")\n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "    parameters = {\n",
    "        \"iris\": {\"full\": \"11\", \"eps\": \"1.111\", \"knn\": \"10\", \"mknn\": \"30\"},\n",
    "        \"madelon\": {\"full\": \"45\", \"eps\": \"4.669\", \"knn\": \"419\", \"mknn\": \"117\"},\n",
    "        \"20newsgroups\": {\"full\": \"35\", \"eps\": \"1946.74\", \"knn\": \"512\", \"mknn\": \"26\"}\n",
    "    }\n",
    "    for graph_type in tqdm(graph_types, desc=\"Processing graph types\"):\n",
    "        for laplacian_type in tqdm(laplacian_types, desc=f\"Processing Laplacian for {graph_type}\", leave=False):\n",
    "            experiments.append({\n",
    "                \"node_label\": label,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[dataset][graph_type],\n",
    "                \"remove_columns\": remove_columns,\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"target_column\": target_column,\n",
    "                \"use_kmean_for_silhouette\": False\n",
    "            })\n",
    "    results = run_experiments(driver, tqdm(experiments, desc=\"Running feature experiments\"))\n",
    "    save_results(results, dataset)\n",
    "    print(f\"Feature experiment completed for {dataset}\")\n",
    "\n",
    "def run_graph_experiment(dataset, node_label, edge_label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    \"\"\"Run graph-based experiments.\"\"\"\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"node_label\": node_label}})\n",
    "    node_file_path = os.path.join(\"datasets\", f\"{dataset}_nodes.csv\")\n",
    "    edge_file_path = os.path.join(\"datasets\", f\"{dataset}_edges.csv\")\n",
    "    create_graph_nodes(node_file_path, driver, node_label)\n",
    "    create_edges(edge_file_path, driver, node_label, edge_label)\n",
    "    print(f\"Graph nodes and edges created for {dataset}\")\n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    for laplacian_type in tqdm(laplacian_types, desc=\"Processing Laplacian types\"):\n",
    "        experiments.append({\n",
    "            \"node_label\": node_label,\n",
    "            \"is_feature_based\": False,\n",
    "            \"graph_type\": \"full\",\n",
    "            \"parameter\": \"3\",\n",
    "            \"remove_columns\": remove_columns,\n",
    "            \"laplacian_type\": laplacian_type,\n",
    "            \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "            \"target_column\": target_column,\n",
    "            \"use_kmean_for_silhouette\": True\n",
    "        })\n",
    "    results = run_experiments(driver, tqdm(experiments, desc=\"Running graph experiments\"))\n",
    "    save_results(results, dataset)\n",
    "    print(f\"Graph experiment completed for {dataset}\")\n",
    "\n",
    "print(\"Experiment functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21381357-eccb-4648-9013-788e1f61bcea",
   "metadata": {},
   "source": [
    "For this demonstration, we will use the **points** dataset with the following data:\n",
    "\n",
    "| points | x_coordinate | y_coordinate | class |\n",
    "|--------|--------------|--------------|-------|\n",
    "| p1     | 1            | 7            | 1     |\n",
    "| p2     | 1            | 6            | 1     |\n",
    "| p3     | 6            | 2            | 2     |\n",
    "| p4     | 8            | 1            | 2     |\n",
    "| p5     | 10           | 2            | 2     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dad2120-f5b8-4345-b16a-b0371a5c111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexes ensured for all datasets.\n",
      "Feature nodes created for points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing graph types:   0%|                                                                    | 0/4 [00:00<?, ?it/s]\n",
      "Processing Laplacian for full:   0%|                                                             | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Processing Laplacian for eps:   0%|                                                              | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Processing Laplacian for knn:   0%|                                                              | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Processing Laplacian for mknn:   0%|                                                             | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Processing graph types: 100%|████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 66.56it/s]\u001b[A\n",
      "Running feature experiments:   0%|                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SimKit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running feature experiments:   0%|                                                               | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimKit initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke function `simkit.experimental_spectralClustering`: Caused by: java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Number (java.lang.String and java.lang.Number are in module java.base of loader 'bootstrap')}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature experiment completed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Now run the modified experiment for the 'points' dataset.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mrun_feature_experiment_modified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m, in \u001b[0;36mrun_feature_experiment_modified\u001b[1;34m(dataset, label, remove_columns, number_of_eigenvectors, target_column)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m laplacian_type \u001b[38;5;129;01min\u001b[39;00m tqdm(laplacian_types, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Laplacian for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m         experiments\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_feature_based\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_kmean_for_silhouette\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         })\n\u001b[1;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning feature experiments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m save_results(results, dataset)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature experiment completed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 132\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m(driver, experiments)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(experiments, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    108\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m    WITH simkit.experimental_spectralClustering(\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m        node_label: $node_label,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124m           result.adjusted_rand_index_time AS adjusted_rand_index_time\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 132\u001b[0m     data, duration, memory_used, cpu_used \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     simkit_result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilhouette_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhouette_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrand_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrand_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu_used\u001b[39m\u001b[38;5;124m\"\u001b[39m: cpu_used\n\u001b[0;32m    143\u001b[0m     }\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_feature_based\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mrun_query\u001b[1;34m(driver, query, parameters)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m    104\u001b[0m     result \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(query, parameters)\n\u001b[1;32m--> 105\u001b[0m     record \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     data \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mif\u001b[39;00m record \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    107\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:485\u001b[0m, in \u001b[0;36mResult.single\u001b[1;34m(self, strict)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39mnon_concurrent_method\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingle\u001b[39m(\u001b[38;5;28mself\u001b[39m, strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mOptional[Record]:\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Obtain the next and only remaining record or None.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    Calling this method always exhausts the result.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m        * Can raise :exc:`.ResultConsumedError`.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:318\u001b[0m, in \u001b[0;36mResult._buffer\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    317\u001b[0m record_buffer \u001b[38;5;241m=\u001b[39m deque()\n\u001b[1;32m--> 318\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\work\\result.py:270\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:850\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m    847\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    848\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    849\u001b[0m )\n\u001b[1;32m--> 850\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\io\\_bolt4.py:368\u001b[0m, in \u001b[0;36mBolt4x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m BoltStates\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke function `simkit.experimental_spectralClustering`: Caused by: java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Number (java.lang.String and java.lang.Number are in module java.base of loader 'bootstrap')}"
     ]
    }
   ],
   "source": [
    "# Run Feature Experiment on the points dataset\n",
    "dataset = \"points\"\n",
    "params = {\n",
    "    \"label\": \"PointNode\",\n",
    "    \"remove_columns\": \"\",  # Don't remove the id column for simkit; scikit-learn will drop it later.\n",
    "    \"number_of_eigenvectors\": 2,  # Two eigenvectors for two features.\n",
    "    \"target_column\": \"class\"\n",
    "}\n",
    "\n",
    "# Update the default parameters to use string values for the \"points\" dataset.\n",
    "default_parameters = {\n",
    "    \"iris\": {\"full\": 11, \"eps\": 1.111, \"knn\": 10, \"mknn\": 30},\n",
    "    \"madelon\": {\"full\": 45, \"eps\": 4.669, \"knn\": 419, \"mknn\": 117},\n",
    "    \"20newsgroups\": {\"full\": 35, \"eps\": 1946.74, \"knn\": 512, \"mknn\": 26},\n",
    "    \"points\": {\"full\": 1, \"eps\": 5, \"knn\": 2, \"mknn\": 2}\n",
    "}\n",
    "\n",
    "def run_feature_experiment_modified(dataset, label, remove_columns, number_of_eigenvectors, target_column):\n",
    "    delete_all_nodes(driver)\n",
    "    delete_all_indexes(driver)\n",
    "    ensure_indexes(driver, {dataset: {\"label\": label}})\n",
    "    file_path = os.path.join(\"datasets\", f\"{dataset}.csv\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    create_feature_nodes(data, driver, label)\n",
    "    print(f\"Feature nodes created for {dataset}\")\n",
    "    experiments = []\n",
    "    laplacian_types = [\"sym\", \"rw\"]\n",
    "    graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "    parameters = default_parameters\n",
    "    for graph_type in tqdm(graph_types, desc=\"Processing graph types\"):\n",
    "        for laplacian_type in tqdm(laplacian_types, desc=f\"Processing Laplacian for {graph_type}\", leave=False):\n",
    "            experiments.append({\n",
    "                \"node_label\": label,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[dataset][graph_type],\n",
    "                \"remove_columns\": remove_columns,\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"target_column\": target_column,\n",
    "                \"use_kmean_for_silhouette\": False\n",
    "            })\n",
    "    results = run_experiments(driver, tqdm(experiments, desc=\"Running feature experiments\"))\n",
    "    save_results(results, dataset)\n",
    "    print(f\"Feature experiment completed for {dataset}\")\n",
    "\n",
    "# Now run the modified experiment for the 'points' dataset.\n",
    "run_feature_experiment_modified(dataset, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15291a0-1379-43c9-84d2-7110eca7e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the experiment results.\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "result_files = glob.glob(os.path.join(\"results\", \"*_results.csv\"))\n",
    "df_list = []\n",
    "for file in result_files:\n",
    "    temp = pd.read_csv(file)\n",
    "    # Extract dataset name from filename (assumes filename like \"points_results.csv\")\n",
    "    dataset_name = os.path.basename(file).split(\"_results.csv\")[0]\n",
    "    temp['dataset'] = dataset_name.capitalize()\n",
    "    df_list.append(temp)\n",
    "if df_list:\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    display(combined_df.head())\n",
    "\n",
    "    # Plot a simple boxplot for total time comparison between SimKit and scikit-learn\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.boxplot(data=combined_df[['total_time', 'sklearn_total_time']])\n",
    "    plt.title(\"Total Time Comparison: SimKit vs. scikit-learn\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcebfdd5-3c0c-4ffa-af13-9a34cc2e0f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc8414-c6eb-49e1-8c05-ab35ffb0e9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129e633-f856-4e77-b0ed-2365c0c6f2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc7c05-5926-4f2a-b091-50820e6237ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f8a1c-2069-4520-98e2-82709ab47bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657879d-8163-41fb-be2b-489934ced8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c8557-53cc-452f-b9b1-bc2856626c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
